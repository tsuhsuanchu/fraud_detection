{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/debbiechu/fraud-detection-for-credit-card-transaction-record?scriptVersionId=174770747\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Purpose\n\nUtilize unsupervised and supervised learning techniques to detect fraud transactions. Goal is to **minimize the false negatives** as much as possible, because misclassifying a fraudulent transaction as non-fraudulent is more detrimental. ","metadata":{}},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"# read data\nimport pandas as pd\npd.set_option('display.max_columns', None)\ndf = pd.read_csv('creditcard.csv')\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# no missing values\ndf.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classes\ndf.Class.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the imbalanced data using the first 2 variables\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x='V1', y='V2', hue='Class')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nX = df.drop('Class', axis=1)\nX_notime = X.drop('Time', axis=1)\ny = df[['Class']]\n\n# calculate correlation between variables\ncor = X_notime.corr()\n\n# plot correlation\nplt.figure(figsize=(10, 8))\nsns.heatmap(cor, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Correlation')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the variables have weak negative correlation with each other, while some has moderate positive correlation.","metadata":{}},{"cell_type":"code","source":"# check distribution of all variables\n\nfor column in X_notime.columns:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Class', y=column, data=df)\n    plt.title(f'Boxplot of {column} by Class')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check normality with Dâ€™Agostino-Pearson\nfrom scipy.stats import normaltest\n\nfor column in X_notime.columns:\n    stat, p = normaltest(X_notime[column])\n    print(f'{column}: p={p}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All **not** normal","metadata":{}},{"cell_type":"code","source":"# check statistical difference in distributions between 2 classes with mann-whitney\nfrom scipy import stats\nfrom scipy.stats import mannwhitneyu\n\nfor column in X_notime.columns:\n    class0 = df.loc[df['Class'] == 0, column]\n    class1 = df.loc[df['Class'] == 1, column]\n    stat, p = stats.mannwhitneyu(class0, class1)\n    print(f'{column}: p={p}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2 classes are statistically significantly different across all the variables, except for **V13, V15, V22**. We can use this info for feature selection later.","metadata":{}},{"cell_type":"code","source":"# Check feature importance with RFC and GBC\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# RF classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X, y)\n\n# GB classifier\ngbm = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngbm.fit(X, y)\n\n\n# get individual feature importance\nfeature_importance_rf = pd.DataFrame(rf.feature_importances_,\n                                     index=X.columns,\n                                     columns=['Importance']).sort_values(by='Importance', ascending=False).sort_index()\nfeature_importance_gbm = pd.DataFrame(gbm.feature_importances_,\n                                      index=X.columns,\n                                      columns=['Importance']).sort_values(by='Importance', ascending=False).sort_index()\n\n# Calculate average importance and make into a DataFrame\naverage_importance = (feature_importance_rf['Importance'] + feature_importance_gbm['Importance']) / 2\naverage_importance_df = pd.DataFrame({'Average Importance': average_importance}).sort_values(by='Average Importance', ascending=False)\n\n# Plot average feature importances\nplt.figure(figsize=(10, 15))\nplt.barh(average_importance_df.index, average_importance_df['Average Importance'], color='purple')\nplt.xlabel('Average Importance')\nplt.title('Average Feature Importance - RF and GBC')\nplt.gca().invert_yaxis()  # To display the most important feature on top\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Supervised learning","metadata":{}},{"cell_type":"markdown","source":"### BalancedRandomForestClassifier","metadata":{}},{"cell_type":"code","source":"# train test split\n\nfrom sklearn.model_selection import train_test_split\n\n# train test val split\nX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n\n# Scale data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Fit scaler\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.ensemble import BalancedRandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, brier_score_loss, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\n\nbrf = BalancedRandomForestClassifier(random_state=42)\n\n# define the param grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 3, 5, 10],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# grid search\ngrid_search = GridSearchCV(estimator=brf, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train_scaled, y_train)\ny_pred=grid_search.predict(X_test_scaled)\n\n# Best parameters and best score\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Best ROC AUC on train:\", grid_search.best_score_)\n\n# Get the probabilistic predictions for the positive class\ny_probs = grid_search.predict_proba(X_test_scaled)[:, 1]\n\n# eval\nbrier_score1 = brier_score_loss(y_test, y_probs)\nprint(\"Brier score:\", brier_score1)\nroc_auc1 = roc_auc_score(y_test, y_probs)\nprint(\"ROC AUC on test:\", roc_auc1)\nprint()\nprint('***Classification Report***')\nprint(classification_report(y_test, y_pred))\nreport1 = classification_report(y_test, y_pred, output_dict=True)\nprint()\nprint('***Confusion Matrix***')\nconf_matrix = pd.crosstab(y_test.iloc[:, 0], y_pred, rownames=['Actual'], colnames=['Predicted'])\nprint(sns.heatmap(conf_matrix, annot=True, fmt=\"d\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val set prob class 1\ny_val_probs = grid_search.best_estimator_.predict_proba(X_val_scaled)[:, 1]\n\nfrom sklearn.metrics import precision_recall_curve\n\nprecision, recall, thresholds = precision_recall_curve(y_val, y_val_probs)\n\nplt.figure(figsize=(10, 6))\n\n# Plot histogram to see class 1 prob distribution\nplt.hist(y_val_probs, bins=1000, alpha=0.5, label='Class 1 Probabilities')\nplt.xlabel('Predicted Probability')\nplt.ylabel('Frequency')\nplt.title('Distribution of Predicted Probabilities for Class 0 and Class 1')\nplt.legend(loc='best')\nplt.ylim(0, 500)\nplt.xlim(0, 1)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adjust threshold\nthreshold = 0.24\ny_pred_adj = (y_probs > threshold).astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# eval\nbrier_score1 = brier_score_loss(y_test, y_probs)\nprint(\"Brier score:\", brier_score1)\nroc_auc1 = roc_auc_score(y_test, y_probs)\nprint(\"ROC AUC on test:\", roc_auc1)\nprint()\nprint('***Classification Report***')\nprint(classification_report(y_test, y_pred_adj))\nreport1 = classification_report(y_test, y_pred_adj, output_dict=True)\nprint()\nprint('***Confusion Matrix***')\nconf_matrix = pd.crosstab(y_test.iloc[:, 0], y_pred_adj, rownames=['Actual'], colnames=['Predicted'])\nprint(sns.heatmap(conf_matrix, annot=True, fmt=\"d\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Threshold 0.4: **7** FN **2767** FP\n- Threshold 0.33: **6** FN **4495** FP\n- Threshold 0.24: **5** FN **8053** FP","metadata":{}},{"cell_type":"markdown","source":"When we lower 1 FN, the number of FP doubles.","metadata":{}},{"cell_type":"markdown","source":"### XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\n# train test val split\nX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n\n# Scale data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Fit scaler\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the ratio of class 0 to class 1\ncount_class_0, count_class_1 = y_train.value_counts()\nscale_pos_weight = count_class_0 / count_class_1\n\n# xgbc with adjusted class weight\nxgbc = xgb.XGBClassifier(random_state=42, scale_pos_weight=scale_pos_weight)\n\n# Fit the model with early stopping\neval_set = [(X_val_scaled, y_val)]\nxgbc.fit(X_train_scaled, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=True)\ny_pred = xgbc.predict(X_test_scaled)\n\n# Get the probabilistic predictions for the positive class\ny_probs = xgbc.predict_proba(X_test_scaled)[:, 1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, brier_score_loss, roc_auc_score\n\n# eval\nbrier_score2 = brier_score_loss(y_test, y_probs)\nprint(\"Brier score:\", brier_score2)\nroc_auc2 = roc_auc_score(y_test, y_probs)\nprint(\"ROC AUC on test:\", roc_auc2)\nprint()\nprint('***Classification Report***')\nprint(classification_report(y_test, y_pred))\nreport2 = classification_report(y_test, y_pred, output_dict=True)\nprint()\nprint('***Confusion Matrix***')\nconf_matrix = pd.crosstab(y_test.iloc[:, 0], y_pred, rownames=['Actual'], colnames=['Predicted'])\nprint(sns.heatmap(conf_matrix, annot=True, fmt=\"d\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though overall False N and P are lower, the FN rate is much higher than BRF. We will try lowering the FN to see how much FP will increase for the trade-off.","metadata":{}},{"cell_type":"code","source":"# val set probs\nprobs = xgbc.predict_proba(X_val_scaled)\nprobs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[prob being class 0, prob being class 1]","metadata":{}},{"cell_type":"code","source":"probs_class_1 = probs[:, 1] # class 1 prob\n\nplt.figure(figsize=(10, 6))\n\n# Plot histogram to see class 1 prob distribution\nplt.hist(probs_class_1, bins=1000, alpha=0.5, label='Class 1 Probabilities')\nplt.xlabel('Predicted Probability')\nplt.ylabel('Frequency')\nplt.title('Distribution of Predicted Probabilities for Class 0 and Class 1')\nplt.legend(loc='best')\nplt.ylim(0, 500)\nplt.xlim(0, 0.05)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the threshold with the lowest FN when FP rate doesn't exceed 10% \n\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\nthresholds = [0.001, 0.02, 0.03]\n\nbest_threshold = None\nlowest_FN = np.inf\nmax_allowed_FP = 0.1 * np.sum(y_val.values.ravel() == 0)  # 10% of all true negatives\n\nfor threshold in thresholds:\n    predictions = (probs_class_1 > threshold).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_val.values.ravel(), predictions).ravel()\n    \n    # Check if FP is below 10%\n    if fp <= max_allowed_FP:\n        # If FP is within the limit, best threshold has the smallest FN\n        if fn < lowest_FN:\n            lowest_FN = fn\n            best_threshold = threshold\n\nprint(f\"Best Threshold: {best_threshold}\")\nprint(f\"Lowest FN (within FP constraint): {lowest_FN}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adjust threshold\ny_pred_adj = (y_probs > best_threshold).astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# eval\nbrier_score2 = brier_score_loss(y_test, y_probs)\nprint(\"Brier score:\", brier_score2)\nroc_auc2 = roc_auc_score(y_test, y_probs)\nprint(\"ROC AUC on test:\", roc_auc2)\nprint()\nprint('***Classification Report***')\nprint(classification_report(y_test, y_pred_adj))\nreport2 = classification_report(y_test, y_pred_adj, output_dict=True)\nprint()\nprint('***Confusion Matrix***')\nconf_matrix = pd.crosstab(y_test.iloc[:, 0], y_pred_adj, rownames=['Actual'], colnames=['Predicted'])\nprint(sns.heatmap(conf_matrix, annot=True, fmt=\"d\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the same number of FN = 8, BRF gets 1386 FP, and XGBoost gets 1143, **XGBoost** performs better and is much more efficient.","metadata":{}},{"cell_type":"markdown","source":"### MLP","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ntf.config.list_physical_devices()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop('Class', axis=1)\ny = df[['Class']]\n\n# train test val split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n\n# Scale data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Fit scaler\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.metrics import AUC\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# MLP model\nmodel = Sequential([\n    # 1st layer with 32 neurons\n    Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)), \n    # 2nd layer with 32 neurons\n    Dense(32, activation='relu'), \n    # add dropout for regularization\n    Dropout(0.2),\n    # output layer\n    Dense(1, activation='sigmoid')\n])\n\n# Compile the model (adam lr 0.001 is default)\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy', AUC()])\n\n# save model with lowest val loss\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath='best_MLP_model.h5',\n    save_best_only=True,\n    monitor='val_loss',\n    mode='min',\n    verbose=1\n)\n\n# train model\nepochs_hist = model.fit(X_train_scaled, y_train, epochs=30, batch_size=20, verbose=1, validation_data=(X_val_scaled, y_val), callbacks=[model_checkpoint_callback])\n\n# Plot the train and validation loss\nplt.plot(epochs_hist.history['loss']) # Training loss\nplt.plot(epochs_hist.history['val_loss']) # Validation loss\nplt.title('Model Loss Progression During Training/Validation')\nplt.ylabel('Training and Validation Losses')\nplt.xlabel('Epoch Number')\nplt.legend(['Training Loss', 'Validation Loss'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the saved model\nmodel = load_model('best_MLP_model.h5')\n\n# Eval test set\ntest_loss, test_acc, test_auc = model.evaluate(X_test_scaled, y_test, verbose=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_probs = model.predict(X_val_scaled) # class 1 prob","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Determine optimal threshold based on val prob distribution\n\nplt.figure(figsize=(10, 6))\n\n# Plot histogram to see class 1 prob distribution\nplt.hist(val_probs, bins=1000, alpha=0.5, label='Class 1 Probabilities')\nplt.xlabel('Predicted Probability')\nplt.ylabel('Frequency')\nplt.title('Distribution of Predicted Probabilities for Class 0 and Class 1')\nplt.legend(loc='best')\nplt.ylim(0, 100)\nplt.xlim(0, 1)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the threshold with the lowest FN when FP rate doesn't exceed 10% \n\nfrom sklearn.metrics import confusion_matrix\nthresholds = [0.0001, 0.0009, 0.001, 0.002]\n\nbest_threshold = None\nlowest_FN = np.inf\nmax_allowed_FP = 0.1 * np.sum(y_val.values.ravel() == 0)  # 10% of all true negatives\n\nfor threshold in thresholds:\n    predictions = (val_probs > threshold).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_val.values.ravel(), predictions).ravel()\n    \n    # Check if FP is below 10%\n    if fp <= max_allowed_FP:\n        # If FP is within the limit, best threshold has the smallest FN\n        if fn < lowest_FN:\n            lowest_FN = fn\n            best_threshold = threshold\n\nprint(f\"Best Threshold: {best_threshold}\")\nprint(f\"Lowest FN (within FP constraint): {lowest_FN}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, precision_recall_curve, f1_score\nimport numpy as np\n\n# get probabilites\ny_probs = model.predict(X_test_scaled).ravel()\n\n# Adjust threshold\ny_pred_adj = (y_probs > best_threshold).astype(int)\n\n# eval\nbrier_score = brier_score_loss(y_test, y_probs)\nprint(\"Brier score:\", brier_score)\nroc_auc = roc_auc_score(y_test, y_probs)\nprint(\"ROC AUC on test:\", roc_auc)\nprint()\nprint('***Classification Report***')\nprint(classification_report(y_test, y_pred_adj))\nreport = classification_report(y_test, y_pred_adj, output_dict=True)\nprint()\nprint('***Confusion Matrix***')\nconf_matrix = pd.crosstab(y_test.iloc[:, 0], y_pred_adj, rownames=['Actual'], colnames=['Predicted'])\nprint(sns.heatmap(conf_matrix, annot=True, fmt=\"d\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the same number of FN = 8, BRF gets 1386 FP, and XGBoost gets 1143 FP, MLP gets 1489 FP, **XGBoost** still performs better.","metadata":{}},{"cell_type":"markdown","source":"## Unsupervised learning","metadata":{}},{"cell_type":"markdown","source":"### PCA","metadata":{}},{"cell_type":"code","source":"# train test split\n\nX = df.drop('Class', axis=1)\ny = df[['Class']]\n\nfrom sklearn.model_selection import train_test_split\n\n# train test val split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Fit scaler\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# train set\npca = PCA(.95)\npca.fit(X_train_scaled)\nX_train_reconstruct = pca.inverse_transform(pca.transform(X_train_scaled))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test set\npca = PCA(.95)\npca.fit(X_test_scaled)\nX_test_reconstruct = pca.inverse_transform(pca.transform(X_test_scaled))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate dif between original and recontructed data\n# square them so the dif are positive, easier to compare\n# sum up the squared differences\nreconstruction_error_train = np.sum(np.square(X_train_scaled - X_train_reconstruct), axis=1)\nreconstruction_error_test = np.sum(np.square(X_test_scaled - X_test_reconstruct), axis=1)\nreconstruction_error_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the distribution\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(30, 10))\nplt.hist(reconstruction_error_train, bins=100, alpha=0.75, color='blue', edgecolor='black')\nplt.title('Histogram of Reconstruction Error')\nplt.xlabel('Reconstruction Error')\nplt.ylabel('Frequency')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.ylim(0,600)\nplt.xticks(np.arange(0, 5000, 100))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the tail on the right could be potential anomalies.","metadata":{}},{"cell_type":"markdown","source":"Use train set to determine the threshold","metadata":{}},{"cell_type":"code","source":"# if using 99 percentile as the threshold\nthreshold = np.percentile(reconstruction_error_train, 99)\nanomalies = reconstruction_error_train > threshold\nsum(anomalies) # num of anomalies detected","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# or use a self-defined threshold\nanomalies = reconstruction_error_train > 100\nsum(anomalies)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the threshold with the lowest FN when FP rate doesn't exceed 10% \n\nfrom sklearn.metrics import confusion_matrix\npercentiles = range(95, 100) # we'll try 95 to 99 percentile\nthresholds = [np.percentile(reconstruction_error_train, percentile) for percentile in percentiles]\n\nbest_threshold = None\nlowest_FN = np.inf\nmax_allowed_FP = 0.1 * np.sum(y_train.values.ravel() == 0)  # 10% of all true negatives\n\nfor threshold in thresholds:\n    predictions = np.where(reconstruction_error_train > threshold, 1, 0)\n    tn, fp, fn, tp = confusion_matrix(y_train.values.ravel(), predictions).ravel()\n    \n    # Check if FP is below 10%\n    if fp <= max_allowed_FP:\n        # If FP is within the limit, best threshold has the smallest FN\n        if fn < lowest_FN:\n            lowest_FN = fn\n            best_threshold = threshold\n\nprint(f\"Best Threshold: {best_threshold}\")\nprint(f\"Lowest FN (within FP constraint): {lowest_FN}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use this threshold on the test set\n\ntest_predictions = np.where(reconstruction_error_test > best_threshold, 1, 0)\n\n# eval\nprint('***Classification Report***')\nprint(classification_report(y_test.values.ravel(), test_predictions))\nreport3 = classification_report(y_test.values.ravel(), test_predictions, output_dict=True)\nprint()\nprint('***Confusion Matrix***')\nconf_matrix = pd.crosstab(y_test.values.ravel(), test_predictions, rownames=['Actual'], colnames=['Predicted'])\nprint(sns.heatmap(conf_matrix, annot=True, fmt=\"d\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"FN is too high","metadata":{}},{"cell_type":"markdown","source":"### Autoencoder","metadata":{}},{"cell_type":"code","source":"X = df.drop('Class', axis=1)\ny = df[['Class']]\n\n# train test val split\nX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n\n# scale \nscaler = StandardScaler()\n\n# Fit scaler\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Input, Dense\nfrom keras.models import Model\n\n# Define the strucutre\ninput_layer = Input(shape=(X_train.shape[1],))\nencoder = Dense(32, activation=\"relu\")(input_layer)\ndecoder = Dense(X_train.shape[1], activation=\"sigmoid\")(encoder)\n\n# initiate model\nautoencoder = Model(input_layer, decoder)\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')\n\n# Train the autoencoder\nautoencoder.fit(X_train_scaled, X_train_scaled,\n                epochs=50,\n                batch_size=256,\n                shuffle=True,\n                validation_data=(X_val_scaled, X_val_scaled))\n\n# calculate reconstruction error for val\nreconstructed = autoencoder.predict(X_val_scaled)\nmse_val = np.mean(np.power(X_val_scaled - reconstructed, 2), axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# will use val to determine threshold \nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(30, 10))\nplt.hist(mse_val, bins=100, alpha=0.75, color='blue', edgecolor='black')\nplt.title('Histogram of Reconstruction Error')\nplt.xlabel('Reconstruction Error')\nplt.ylabel('Frequency')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.ylim(0,1000)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the threshold with the lowest FN when FP rate doesn't exceed 10% \n\nfrom sklearn.metrics import confusion_matrix\n\nthresholds = [1.75]\n\nbest_threshold = None\nlowest_FN = np.inf\nmax_allowed_FP = 0.1 * np.sum(y_val.values.ravel() == 0)  # 10% of all true negatives\n\nfor threshold in thresholds:\n    predictions = np.where(mse_val > threshold, 1, 0)\n    tn, fp, fn, tp = confusion_matrix(y_val.values.ravel(), predictions).ravel()\n    \n    # Check if FP is below 10%\n    if fp <= max_allowed_FP:\n        # If FP is within the limit, best threshold has the smallest FN\n        if fn < lowest_FN:\n            lowest_FN = fn\n            best_threshold = threshold\n\nprint(f\"Best Threshold: {best_threshold}\")\nprint(f\"Lowest FN (within FP constraint): {lowest_FN}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate reconstruction error for test\nreconstructed = autoencoder.predict(X_test_scaled)\nmse_test = np.mean(np.power(X_test_scaled - reconstructed, 2), axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use this threshold on the test set\n\ntest_predictions = np.where(mse_test > best_threshold, 1, 0)\n\n# eval\nprint('***Classification Report***')\nprint(classification_report(y_test.values.ravel(), test_predictions))\nreport3 = classification_report(y_test.values.ravel(), test_predictions, output_dict=True)\nprint()\nprint('***Confusion Matrix***')\nconf_matrix = pd.crosstab(y_test.values.ravel(), test_predictions, rownames=['Actual'], colnames=['Predicted'])\nprint(sns.heatmap(conf_matrix, annot=True, fmt=\"d\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"14/(14+84) # FN rate","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Better than PCA","metadata":{}},{"cell_type":"markdown","source":"## Feature selection","metadata":{}},{"cell_type":"markdown","source":"I wil try on XGboost only","metadata":{}},{"cell_type":"code","source":"# drop 'V13','V15','V22' since they are not statisitcally significantly different across classes\nX = df.drop(['Class','V13','V15','V22'], axis=1)\ny = df[['Class']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\n\n# train test val split\nX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n\n# Scale data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Fit scaler\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the ratio of class 0 to class 1\ncount_class_0, count_class_1 = y_train.value_counts()\nscale_pos_weight = count_class_0 / count_class_1\n\n# xgbc with adjusted class weight\nxgbc = xgb.XGBClassifier(random_state=42, scale_pos_weight=scale_pos_weight)\n\n# Fit the model with early stopping\neval_set = [(X_val_scaled, y_val)]\nxgbc.fit(X_train_scaled, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=True)\ny_pred = xgbc.predict(X_test_scaled)\n\n# Get the probabilistic predictions for the positive class\ny_probs = xgbc.predict_proba(X_test_scaled)[:, 1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, brier_score_loss, roc_auc_score\n\n# eval\nbrier_score2 = brier_score_loss(y_test, y_probs)\nprint(\"Brier score:\", brier_score2)\nroc_auc2 = roc_auc_score(y_test, y_probs)\nprint(\"ROC AUC on test:\", roc_auc2)\nprint()\nprint('***Classification Report***')\nprint(classification_report(y_test, y_pred))\nreport2 = classification_report(y_test, y_pred, output_dict=True)\nprint()\nprint('***Confusion Matrix***')\nconf_matrix = pd.crosstab(y_test.iloc[:, 0], y_pred, rownames=['Actual'], colnames=['Predicted'])\nprint(sns.heatmap(conf_matrix, annot=True, fmt=\"d\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Slightly better than before dimension reduction, which was 4FP 17 FN.","metadata":{}},{"cell_type":"code","source":"probs = xgbc.predict_proba(X_val_scaled) # val set probs\nprobs_class_1 = probs[:, 1] # class 1 prob\n\nplt.figure(figsize=(10, 6))\n\n# Plot histogram to see class 1 prob distribution\nplt.hist(probs_class_1, bins=1000, alpha=0.5, label='Class 1 Probabilities')\nplt.xlabel('Predicted Probability')\nplt.ylabel('Frequency')\nplt.title('Distribution of Predicted Probabilities for Class 0 and Class 1')\nplt.legend(loc='best')\nplt.ylim(0, 500)\nplt.xlim(0, 0.05)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the threshold with the lowest FN when FP rate doesn't exceed 10% \n\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\nthresholds = [0.001, 0.002, 0.003, 0.004] \n\nbest_threshold = None\nlowest_FN = np.inf\nmax_allowed_FP = 0.1 * np.sum(y_val.values.ravel() == 0)  # 10% of all true negatives\n\nfor threshold in thresholds:\n    predictions = (probs_class_1 > threshold).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_val.values.ravel(), predictions).ravel()\n    \n    # Check if FP is below 10%\n    if fp <= max_allowed_FP:\n        # If FP is within the limit, best threshold has the smallest FN\n        if fn < lowest_FN:\n            lowest_FN = fn\n            best_threshold = threshold\n\nprint(f\"Best Threshold: {best_threshold}\")\nprint(f\"Lowest FN (within FP constraint): {lowest_FN}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adjust threshold\ny_pred_adj = (y_probs > best_threshold).astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# eval\nbrier_score2 = brier_score_loss(y_test, y_probs)\nprint(\"Brier score:\", brier_score2)\nroc_auc2 = roc_auc_score(y_test, y_probs)\nprint(\"ROC AUC on test:\", roc_auc2)\nprint()\nprint('***Classification Report***')\nprint(classification_report(y_test, y_pred_adj))\nreport2 = classification_report(y_test, y_pred_adj, output_dict=True)\nprint()\nprint('***Confusion Matrix***')\nconf_matrix = pd.crosstab(y_test.iloc[:, 0], y_pred_adj, rownames=['Actual'], colnames=['Predicted'])\nprint(sns.heatmap(conf_matrix, annot=True, fmt=\"d\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Result comparison:\n- before DR: **8** FN + **1143** FP\n- after DR: **7** FN + **767** FP\n\nDimension reduction further improved the predictive ability of xgboost.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"Supervised learning method, specifically **XGboost**, overall performs better, adding **dimension reduction** the model was able to make more accurate predictions. **Autoencoder** is the best among all the unsupervised techniques I tried, but still not performing nearly as well as the supervised methods.","metadata":{}}]}